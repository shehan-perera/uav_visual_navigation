{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from preprocessing import PreprocessData\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read images\n",
    "def read_image(path_to_img: str, array: bool=True):\n",
    "    '''\n",
    "    args:\n",
    "        path_to_img [string] -- path to the image\n",
    "    function:\n",
    "        opens and returns the image as a numpy array\n",
    "    returns:\n",
    "        image array\n",
    "    interactions:\n",
    "        can be called by split_image    \n",
    "    '''\n",
    "    if array:\n",
    "        return np.array(Image.open(path_to_img), dtype=np.uint8)\n",
    "    else:\n",
    "        return Image.open(path_to_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split_image(path_to_image: str, path_to_map: str, tile_shape: int=256, max_patches: int=30, combine: bool=True):\n",
    "    '''\n",
    "    args:\n",
    "        path_to_img [string] -- path to the image\n",
    "        tile_shape [int] -- the size of each of the tiles [tile_shape x tile_shape]\n",
    "    function:\n",
    "        tiles the image and returns a [tile_shape, tile_shape, num_tiles] array\n",
    "        THIS FUNCTION IS USED TO CREATE IMAGE PATCHES FOR TRAINING, NOT TO BE USED FOR INFERENCE\n",
    "        IF ANYTHING THIS CAN BE SEEN AS A PRETRAINING, BECAUSE WE ARE ONLY USING THE PATCHES, AND ONCE ITS PRETRAIND\n",
    "            WE CAN THE ADD AN ADDITIONAL TERM WHERE WE NOT ONLY WANT THE PATCHES TO BE PREDICTED CORRECTLY WE WILL\n",
    "            COMBINE THE PATCHES AND ADD A L1 LOSS TERM BETWEEN THE RECONSTUCTED MAP AND THE REAL MAP. \n",
    "    returns:\n",
    "        returns a [tile_shape, tile_shape, num_tiles] array\n",
    "    interactions:\n",
    "        called by build_dataset()  \n",
    "    '''\n",
    "    # open image\n",
    "    image = read_image(path_to_image, array=True)   \n",
    "    map_img = read_image(path_to_map, array=True)\n",
    "    \n",
    "    # if combine is true combine the map and image data into single array\n",
    "    # useful to have during but can be turned off at inference\n",
    "    if combine:\n",
    "        image = np.dstack((image, map_img))\n",
    "        \n",
    "    # resize the images so this function and inference_split_images will be the same\n",
    "    resize_row = np.around(np.divide(image.shape[0], tile_shape)).astype(np.int) \n",
    "    resize_col = np.around(np.divide(image.shape[1], tile_shape)).astype(np.int) \n",
    "    resized_image = cv2.resize(image, (resize_col*tile_shape, resize_row*tile_shape), cv2.INTER_CUBIC)\n",
    "                          \n",
    "    # return the patches\n",
    "    return extract_patches_2d(resized_image, patch_size=(tile_shape, tile_shape), max_patches=30, random_state=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tiles(tiles_array, image_path: str, map_path: str, tiles_dir_path: str):\n",
    "    '''\n",
    "    args: \n",
    "        tiles_array -- the tiles from train_split_image or from inference_split_image\n",
    "        image_path -- the path used for the image \n",
    "        map_path -- the path used for the map\n",
    "        tiles_dir_path -- the path to the directory that stores the tile data. dir should have 2 folders \"image_tiles\", \"map_tiles\"\n",
    "    function:\n",
    "        the image tiles and the map tiles will be saved into a directory using the same name from the image_path\n",
    "    returns:\n",
    "        none\n",
    "    interactions:\n",
    "        called by build_dataset()    \n",
    "    '''\n",
    "    \n",
    "    # grab the iamge tiles\n",
    "    image_tiles = tiles_array[:, :, :, 0:4]\n",
    "    map_tiles = tiles_array[:, :, :, 4:]\n",
    "    \n",
    "    # get the name from the image path\n",
    "    image_name = os.path.basename(image_path).split('.')[0]\n",
    "    map_name = os.path.basename(map_path).split('.')[0]\n",
    "    \n",
    "    # save the image files\n",
    "    for i in range(image_tiles.shape[0]):\n",
    "        current_image_name = image_name + '_tile_' + str(i) + '.png'\n",
    "        image_save_path = os.path.join(tiles_dir_path, 'image_tiles', current_image_name)\n",
    "        cv2.imwrite(image_save_path, image_tiles[i, :, :, :])\n",
    "        \n",
    "    # save the map files\n",
    "    for i in range(map_tiles.shape[0]):\n",
    "        current_map_name = image_name + '_tile_' + str(i) + '.png'\n",
    "        map_save_path = os.path.join(tiles_dir_path, 'map_tiles', current_map_name)\n",
    "        cv2.imwrite(map_save_path, map_tiles[i, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the inference split, the function should reshape the file while maintating the aspect ratio. \n",
    "def inference_split_image(path_to_image: str, path_to_map: str, tile_shape: int=256, combine: bool=True):\n",
    "    '''\n",
    "    args:\n",
    "        path_to_img [string] -- path to the image\n",
    "        tile_shape [int] -- the size of each of the tiles tile\n",
    "    function:\n",
    "        used for resizing the input image while maintaining the aspect ratio for inference\n",
    "        this function is different from train_split_image b/c this one can be reconstructed back into the original image\n",
    "        if combine is True, the tiled_data contains the image and the map tiles together to make 8 channels \n",
    "    return:\n",
    "        tiled_data [num_tiles, height, width, 8] if combine is True else [num_tiles, height, width, 4]\n",
    "    interactions:\n",
    "        called by build_dataset()    \n",
    "    '''\n",
    "    # open image as array image\n",
    "    image = read_image(path_to_image, array=True)\n",
    "    map_img = read_image(path_to_map, array=True)\n",
    "    \n",
    "    # if combine is true combine the map and image data into single array\n",
    "    # useful to have during but can be turned off at inference\n",
    "    if combine:\n",
    "        image = np.dstack((image, map_img))\n",
    "            \n",
    "    # resize keeping aspect ratio (maybe)\n",
    "    resize_row = np.around(np.divide(image.shape[0], tile_shape)).astype(np.int) \n",
    "    resize_col = np.around(np.divide(image.shape[1], tile_shape)).astype(np.int) \n",
    "    resized_image = cv2.resize(image, (resize_col*tile_shape, resize_row*tile_shape), cv2.INTER_CUBIC)\n",
    "        \n",
    "    # image to array\n",
    "    num_tiles = resize_row * resize_col\n",
    "    tiled_data = np.zeros((num_tiles, tile_shape, tile_shape, image.shape[-1]), dtype=np.uint8)\n",
    "    \n",
    "    # split the image into tile_shape x tile_shape tiles\n",
    "    for row in range(resize_row):\n",
    "        for col in range(resize_col):\n",
    "            count = row * col \n",
    "            start_row = row * tile_shape\n",
    "            end_row = (row * tile_shape) + tile_shape\n",
    "            start_col = col + tile_shape\n",
    "            end_col = (col + tile_shape) + tile_shape\n",
    "            tiled_data[count, :, :, :] = resized_image[start_row:end_row, start_col:end_col, :]\n",
    "    \n",
    "    #return tiles\n",
    "    return tiled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(path_to_data_folder: str, inference: bool=True):\n",
    "    '''\n",
    "    args:\n",
    "        path_to_data_folder [string] -- the path to the folder that contains the folders \"map_image_data\" and \"sat_image_data\" and \"maps_data\" and \"tiled_data\"\n",
    "    function:\n",
    "        builds the data set\n",
    "        first will use preprocessing.ProcessData to organize the data \n",
    "        next it will split the images into tiles and saves it into tiled_data/inference_tiles if inference is True else to tiled_data/training_files\n",
    "    returns:\n",
    "        None\n",
    "    interactions:\n",
    "        acts as main\n",
    "    '''\n",
    "    print('[info] -- started building dataset')\n",
    "    \n",
    "    # move the data from \"maps_data\" to \"map_image_data\" and \"sat_image_data\"\n",
    "    original_data_path = os.path.join(path_to_data_folder, 'maps_data')\n",
    "    map_image_data_path = os.path.join(path_to_data_folder, 'map_image_data')\n",
    "    sat_image_data_path = os.path.join(path_to_data_folder, 'sat_image_data')\n",
    "    \n",
    "    print('[info] -- organizing data')\n",
    "    # if map_image_data or sat_image_data are not empty dont preprocess\n",
    "    if len(os.listdir(map_image_data_path)) != 0:\n",
    "        pass\n",
    "    else:\n",
    "        # if the folders dont exist create them\n",
    "        os.mkdir(map_image_data_path)\n",
    "        os.mkdir(sat_image_data_path)\n",
    "        PreprocessData.move_files(data_path=original_data_path, sat_save_dir=sat_image_data_path, map_save_dir=map_image_data_path)\n",
    "    \n",
    "    # get the csv file of the map and image data pairs \n",
    "    data_csv =  PreprocessData.create_data_csv(path_to_data_folder)\n",
    "      \n",
    "    if inference:\n",
    "        print('[info] -- creating tiles for inference')\n",
    "        # image save path\n",
    "        save_dir = os.path.join(path_to_data_folder, 'tiled_data/inference_tiles')\n",
    "        \n",
    "        # create the directories to store the image data in\n",
    "        os.mkdir(os.path.join(save_dir, 'image_tiles'))\n",
    "        os.mkdir(os.path.join(save_dir, 'map_tiles'))\n",
    "        \n",
    "        for idx in tqdm(range(len(data_csv))):\n",
    "            current_path_to_image = data_csv['sat_path'][idx]\n",
    "            current_path_to_map = data_csv['map_path'][idx]\n",
    "            current_tile = inference_split_image(current_path_to_image, current_path_to_map, tile_shape=256, combine=True)\n",
    "            save_tiles(current_tile, current_path_to_image, current_path_to_map, save_dir)\n",
    "    else:\n",
    "        print('[info] -- creating tiles for training')\n",
    "        # image save path\n",
    "        save_dir = os.path.join(path_to_data_folder, 'tiled_data/training_tiles')\n",
    "        \n",
    "        # create the directories to store the image data in\n",
    "        os.mkdir(os.path.join(save_dir, 'image_tiles'))\n",
    "        os.mkdir(os.path.join(save_dir, 'map_tiles'))\n",
    "        \n",
    "        for idx in tqdm(range(len(data_csv))):\n",
    "            current_path_to_image = data_csv['sat_path'][idx]\n",
    "            current_path_to_map = data_csv['map_path'][idx]\n",
    "            current_tile = train_split_image(current_path_to_image, current_path_to_map, tile_shape=256, max_patches=30, combine=True)\n",
    "            save_tiles(current_tile, current_path_to_image, current_path_to_map, save_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_dataset('../../../data/', inference=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # intialize parser\n",
    "    parser = argparse.ArgumentParser(description='Information to build dataset')\n",
    "    \n",
    "    # arguments \n",
    "    parser.add_argument('data_folder', type=str, help='path to the folder that contains the data folder called \"maps_data')\n",
    "    parser.add_argument('inference', type=bool, help='true indicates budiling inference data and false indicates building trainig data')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # build dataset\n",
    "    build_dataset(args.data_folder, args.inference)\n",
    "    \n",
    "    print('[info] -- finished building dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # path to data folder\n",
    "# data_path = '../data/maps_data/'\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# x,y = PreprocessData.split_category(data_path)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# PreprocessData.move_files(data_path, sat_save_dir='../data/sat_image_data/', map_save_dir='../data/map_image_data/')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# h = PreprocessData.create_data_csv('../data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = inference_split_image(data_csv['sat_path'][0], data_csv['map_path'][0])\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tiles(e, data_csv['sat_path'][0], data_csv['map_path'][0], 'NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join('h', 'j', 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv['sat_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv['map_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS AS MAIN\n",
    "\n",
    "\n",
    "# get the csv files from preprocessdata class\n",
    "data_csv = PreprocessData.create_data_csv('../../../data/')\n",
    "\n",
    "rgb_patch = train_split_image(data_csv['sat_path'][0])\n",
    "map_patch = train_split_image(data_csv['map_path'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rgb_patch[9, :, :, 0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(map_patch[9, :, :, 0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_images(data_csv['sat_path'][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(patch[7, :, :, 0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1680/256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "256*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(767/256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = inference_split_image(data_csv['sat_path'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "256*256*21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1680/256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
